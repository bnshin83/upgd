# Analysis Notes - November 7, 2025

## Task Configuration: Input Permuted MNIST

### Dataset Parameters
- **Total samples**: 1,000,000
- **Task change frequency**: 5,000 steps
- **Number of tasks**: 200 (1,000,000 / 5,000)
- **Batch size**: 1
- **Shuffle mechanism**: PyTorch DataLoader with `shuffle=True` (random per epoch)

### Data Shift
The "shift" for shuffling is **5,000 steps**, which is the `change_freq` parameter where the input permutation changes. Each permutation is applied for 5,000 consecutive samples.

Location: `/scratch/gautschi/shin283/upgd/core/task/input_permuted_mnist.py:14`

---

## Network Architecture

### FullyConnectedReLUWithHooks
- **Layer 1**: 784 � 300 neurons (ReLU)
- **Layer 2**: 300 � 150 neurons (ReLU)
- **Layer 3**: 150 � 10 neurons (output)
- **Total hidden units**: 450 neurons
- **Total parameters**: 287,717

Location: `/scratch/gautschi/shin283/upgd/core/network/fcn_relu.py:39-49`

---

## Utility L1 Norm Analysis (First Task: 0-5000 steps)

### Observed Pattern
Initial utility graph shows three distinct phases:

#### Phase 1: Rapid Accumulation (0-500 steps)
- **Peak at ~500 steps**: L1 norm reaches ~141,500
- Network is in fast learning mode with large gradients
- Weights rapidly accumulate utility as they become important for the first permutation
- This is the "discovery phase" - network identifies which weights matter

#### Phase 2: Convergence & Decay (500-2000 steps)
- **Decline from peak**: ~141,500 � ~141,200
- Network is converging on the task solution
- Gradients get smaller as loss decreases
- Utility is based on gradient information (first-order utility), so smaller gradients � lower utility updates
- Exponential moving average (beta_utility = 0.9999) causes old high utilities to slowly decay
- This is the "refinement phase"

#### Phase 3: Equilibrium (2000-5000 steps)
- **Stable plateau**: ~141,200
- Network has converged on this permutation
- Small, stable gradients � stable utility values
- Task is learned, weights have settled into optimal values
- Utility reaches steady state where new updates balance EMA decay

### Utility Saturation Analysis
- **Maximum possible L1 norm**: 287,717 (all utilities = 1)
- **Observed L1 norm**: ~141,500
- **Average utility per parameter**: 141,500 / 287,717 H **0.492 (49.2%)**

This ~49% saturation is healthy for continual learning:
1. Room to grow - still have ~50% utility capacity for future tasks
2. Not oversaturated - preventing utility collapse
3. Protection active - utility gating mechanism working

### Why Utility Follows Learning Curve
The utility L1 norm follows the learning curve because:
- **High when learning is active** (large weight updates needed)
- **Low when converged** (small weight updates needed)

This pattern should repeat 200 times across all tasks.

### Multi-Norm Analysis: Understanding Utility Distribution

#### Norm Definitions and Intuition

**L1 Norm (Manhattan distance):**
```
||x||₁ = |x₁| + |x₂| + ... + |xₙ|
```
- Sum of absolute values
- Democratic: every element contributes equally
- In our case: Total utility across ALL parameters

**L2 Norm (Euclidean distance):**
```
||x||₂ = √(x₁² + x₂² + ... + xₙ²)
```
- "Balanced" norm - rotation invariant, treats all dimensions equally
- More sensitive to larger values than L1
- Quadratic penalty balances concern for all weights

**Higher-order norms (L4, L5, L10):**
```
||x||ₚ = ᵖ√(|x₁|ᵖ + |x₂|ᵖ + ... + |xₙ|ᵖ)
```
- Increasingly dominated by largest values
- As p increases, small values contribute less
- L10 almost exclusively reflects the highest utilities

**Key intuition:** Higher order norms focus more on the "biggest number portion" and ignore smaller values.

#### Observed Pattern (Steps 0-500)

**L2, L4, L5, L10 behavior:**
- All **drop significantly** from step 0 to ~300
- L5 drops from ~9.5 to ~6.5
- Indicates the **largest utility values are decreasing**

**L1 behavior:**
- **Rises** from ~140,000 to ~141,500 (peak at step 500)
- Then gradually decreases to ~141,400
- Indicates **total utility increasing** despite peak utilities dropping

**This apparent contradiction reveals utility redistribution!**

#### Interpretation: Utility Democratization

**What's happening (0-300 steps):**

```python
# Step 0: Few neurons with high utility
utilities = [0.95, 0.90, 0.85, 0.80, 0.15, 0.10, 0.05, ...]
L1 = 140,000
L5 dominated by [0.95, 0.90, 0.85] → HIGH (~9.5)
Neurons with utility > 0.7: ~1,000 neurons

# Step 300-500: Distribution spreading
utilities = [0.70, 0.68, 0.65, 0.60, 0.55, 0.50, 0.48, 0.45, 0.42, ...]
L1 = 141,500 (HIGHER!)
L5 less dominated → LOWER (~6.5)
Neurons with utility > 0.7: ~500 neurons (FEWER!)
But MANY MORE neurons now in 0.4-0.7 range
```

**The dynamics:**

1. **High utilities (0.8-1.0) decreasing**:
   - Initial "winner" neurons calming down
   - Their gradients shrink as they converge on optimal values

2. **Mid utilities (0.4-0.7) increasing**:
   - More neurons becoming moderately important
   - Previously dormant neurons waking up and contributing

3. **Fewer neurons with extreme utility**:
   - Distribution becomes more uniform
   - Variance/spread decreasing

4. **L1 still rises because**:
   ```
   Lost from high utilities:  -500 neurons × 0.3 drop   = -150 in L1
   Gained from mid utilities: +5000 neurons × 0.4 rise = +2000 in L1
   Net effect: L1 increases by ~1,500!
   ```

#### Why This Happens

**Phase 1 (0-100 steps): Initial winners**
- Network rapidly identifies a small set of highly useful weights
- These get very large gradients → utility approaches 1.0
- Most other weights still have low utility

**Phase 2 (100-300 steps): Redistribution**
- Initial winners converge → their gradients decrease → utility growth slows
- Network discovers it needs MORE neurons for robust learning
- Previously inactive neurons start receiving gradients
- Utility spreads from few dominant neurons to many contributors

**Phase 3 (300-500 steps): Continued spreading**
- More neurons continue joining with moderate utility (0.4-0.6)
- L1 peaks as maximum number of neurons are moderately active
- Distribution becomes more uniform and stable

#### Implications for Continual Learning

**This redistribution is highly desirable:**

1. **Distributed representations**: Not relying on few critical neurons (robust)
2. **Network plasticity maintained**: More neurons participating actively
3. **Balanced protection**: Medium utilities (0.4-0.7) provide moderate protection without complete freezing
4. **Room for adaptation**: Neurons not saturated at utility = 1.0, can still adjust

**ReLU sparsity connection:**
- ~50% average utility (141,400 / 287,717) partly reflects ReLU-induced sparsity
- Dead neurons (output = 0) → no gradients → accumulate little/no utility
- Active neurons accumulate utility based on gradient magnitude
- Different neurons are active on different inputs, creating a spectrum of utility values

**The ~50% L1 saturation represents:**
- NOT a binary split (50% at utility=1.0, 50% at utility=0)
- Rather a continuous distribution across [0, 1]
- Healthy balance for first task of 200 tasks

---

## Gating Mechanisms and Curvature Computation

### Two Different "Gating" Mechanisms

#### 1. GateLayer (Network Architecture)
**Location**: `/scratch/gautschi/shin283/upgd/core/network/gate.py:10-11`

```python
def forward(self, input):
    return input * self.weight
```

**Effect on curvature**:  **YES, it affects curvature computation**
- GateLayer is part of the model's forward pass
- When computing curvature, code runs `model(inputs)` which includes GateLayer
- GateLayer's learned weights scale activations, changing the loss landscape
- Therefore, curvature values reflect the current GateLayer state

#### 2. Optimizer Gating Factor (Update Rule)
**Location**: `/scratch/gautschi/shin283/upgd/core/optim/weight_upgd/input_aware.py:332-346`

```python
gating_factor = torch.clamp(1 - scaled_utility * lambda_reg, min=0.0)
```

**Effect on curvature**: L **NO, it does NOT affect curvature computation**
- This gating factor only affects parameter updates in `optimizer.step()`
- Curvature is computed during forward/backward passes, before `optimizer.step()`
- This gating mechanism is downstream of curvature computation

### Curvature Computation Flow
```python
# Curvature computation (in eval mode):
model.eval()
outputs = model(inputs + v)  # � GateLayer runs here (affects curvature)
loss = criterion(outputs, targets)
grad = autograd.grad(loss, inputs)  # Computes input curvature

# Later, in optimizer.step():
gating_factor = 1 - scaled_utility * lambda_reg  # � Does NOT affect curvature
p.data.add_((grad + noise) * gating_factor)  # Updates happen after
```

---

## Dead Neurons Analysis

### Dead Neuron Computation

#### Hook Implementation
**Location**: `/scratch/gautschi/shin283/upgd/core/network/fcn_relu.py:60-61`

```python
def activation_hook(self, name, module, inp, out):
    self.activations[name] = torch.sum(out == 0.0).item()
```

#### Step-by-Step Computation

**Step 1**: Hook captures zeros per layer
```python
# After forward pass on ONE sample (batch_size=1)
out_layer1 = tensor([[0.0, 2.3, 0.0, ..., 1.5]])  # [1, 300]
self.activations['act_1'] = torch.sum(out_layer1 == 0.0).item()
# Example: 160 (160 neurons output 0.0 for this input)

out_layer2 = tensor([[1.2, 0.0, 0.0, ..., 0.5]])  # [1, 150]
self.activations['act_2'] = torch.sum(out_layer2 == 0.0).item()
# Example: 74 (74 neurons output 0.0 for this input)
```

**Step 2**: Sum across layers
```python
n_dead_units = 0
for _, value in self.learner.network.activations.items():
    n_dead_units += value  # 160 + 74 = 234

n_dead_units_per_step.append(n_dead_units / self.learner.network.n_units)
# 234 / 450 = 0.52 = 52%
```

### Critical Distinctions

#### What it IS measuring:
-  "On this specific input sample, X% of neurons output 0"
-  Measured per individual input sample (batch_size=1)
-  Averaged across all steps when displayed in graphs

#### What it is NOT measuring:
- L Not: "X% of neurons are permanently dead"
- L Not: "X% of neurons never fire"
- L Not averaged over multiple samples simultaneously
- L Not a fixed set of neurons

### Relationship to Utility

**Dead neuron on one sample ` Zero utility globally**

A neuron can be:
- Dead on input A (ReLU output = 0)
- Alive on input B (ReLU output > 0)

Example: A neuron that fires on only 30% of inputs:
- Contributes to "dead neuron" statistic (dead on 70% of samples)
- But **still accumulates utility** from the 30% where it's active

The network doesn't have two disjoint sets (48% alive / 52% dead). Instead:
- Some neurons: Always active (high utility)
- Some neurons: Active on 50-80% of samples (medium utility)
- Some neurons: Active on 20-50% of samples (low utility)
- Some neurons: Rarely/never active (near-zero utility)

**The 52% dead per batch is an average across all neurons and all inputs**, not a fixed set of dead neurons.

---

## Dead Neurons at Initialization (First 500 Steps)

### Initial Value: ~48% Dead at Step 0

#### Random Initialization Effect
At the very first forward pass (before any training):
- Weights are randomly initialized (He/Xavier initialization)
- For ReLU: `output = max(0, w�x + b)`
- If `w�x + b < 0`, the neuron outputs 0 (dead)
- With **zero-centered random initialization**, roughly ~50% of pre-activations are negative
- Therefore, **~48% dead is expected at random initialization**

### Learning Dynamics (Steps 0-500)

#### Phase 1: Initial Spike (Steps 0-50)
- **48% � 49%**: Dead neurons slightly increase
- Some neurons are "dying" as the network starts learning
- Network is discovering which neurons are not useful for the first task

#### Phase 2: Plateau (Steps 50-150)
- **~49% stable**: Network has settled into a working configuration
- This matches the first task learning from utility graph

#### Phase 3: Gradual Decrease (Steps 150-500)
- **49% � 45.5%**: Dead neurons are **decreasing**
- More neurons are becoming active as the network learns
- **This is a GOOD sign** - network is utilizing more capacity, not losing it!

### Key Insight: Maintained Plasticity

The decrease from 49% � 45.5% means:
-  Network is **activating more neurons** as it learns
-  **Plasticity is maintained** - neurons can be "revived"
-  Not experiencing the catastrophic "dying ReLU" problem
-  UPGD algorithm is successfully allowing the network to adapt and use more capacity

---

## Does Gating Cause the Utility Norm Patterns?

### The Question

Looking at the utility norm behavior:
- **L1 norm**: Rises from ~140,000 to ~141,500 by step 500, then gradually saturates
- **Higher-order norms (L5)**: Drop rapidly from ~9.5 to ~6.5 by step 300, then saturate immediately

**Does the gating mechanism `(1 - scaled_utility)` cause these patterns?**

### Base UPGD Gating Mechanism

**Location**: `/scratch/gautschi/shin283/upgd/core/optim/weight_upgd/first_order.py:120-124`

```python
scaled_utility = torch.sigmoid_((avg_utility / bias_correction) / global_max_util)
gating_factor = 1 - scaled_utility
update = (grad + noise) * gating_factor
```

**Purpose of gating**:
- High utility parameters (converged, important for learned task) → gating ≈ 0 → small updates (protected)
- Low utility parameters (not yet important) → gating ≈ 1 → large updates (plastic)
- **Prevents catastrophic forgetting**: High-utility neurons are close to convergence; if updated too much, they forget what they learned

### Analysis: Gating Does NOT Cause the Patterns

**Key insight**: Gating affects weight updates, not gradient computation.

#### The Causal Chain

```python
# Step t:
# 1. Compute gradient from loss (independent of gating)
grad[t] = ∂L/∂θ[t]  # Depends on: current weights, current loss, backprop

# 2. Compute utility from gradient history
utility[t] = 0.9999 * utility[t-1] + 0.0001 * |grad[t] * weight[t]|

# 3. Apply gating to updates
gating = 1 - scaled_utility
θ[t+1] = θ[t] - lr * grad[t] * gating  # Gating affects weights

# Step t+1:
# Gradient still computed from loss, not gating
grad[t+1] = ∂L/∂θ[t+1]  # New gradient from new weights
```

**Gating's indirect effect**:
- Slows convergence for high-utility weights (smaller updates)
- Could theoretically keep gradients larger for longer
- But this would **increase** utility for protected parameters, not create redistribution

#### What Actually Causes the Patterns

**The L1 rise + L5 drop is caused by natural learning dynamics:**

**Phase 1 (0-100 steps): Initial Winners**
```python
# Random initialization → some neurons fire strongly on early inputs
neurons[0:1000] receive gradients ≈ 0.1 (initial winners)
neurons[1000:end] receive gradients ≈ 0.01 (inactive)

# Utility accumulates rapidly for winners
utility[0:1000] → 0.85 (approaching saturation)
utility[1000:end] → 0.15 (low)

# Gating protects the winners
gating[0:1000] = 1 - 0.85 = 0.15 (only 15% of update applied)
gating[1000:end] = 1 - 0.15 = 0.85 (85% of update applied)

L5 = HIGH (~9.5) (dominated by top 1000 neurons at utility ~0.85)
L1 = moderate (~140,000)
```

**Phase 2 (100-500 steps): Natural Gradient Redistribution**

The network's loss landscape naturally evolves:

```python
# As loss decreases, initial winners approach convergence
# Their gradients naturally shrink (closer to local minimum)
neurons[0:1000]: gradients 0.1 → 0.05 (converging, despite gating slowing them)
utility[0:1000]: 0.85 → 0.65 (shrinking grads + EMA decay)

# Backprop discovers more neurons needed for robust solution
# Previously dormant neurons start receiving gradients (natural gradient flow)
neurons[1000:15000]: gradients 0.01 → 0.04 (waking up)
utility[1000:15000]: 0.15 → 0.50 (accumulating)

# Result: Utility redistribution
L5 = DROPS to ~6.5 (no extreme values, less concentration)
L1 = RISES to ~141,500 (many more neurons moderately active)
```

**Why high-utility neurons converge despite gating**:

Even with protection, high-utility neurons still receive small updates:
```python
# High utility neuron (u = 0.85):
gating = 1 - 0.85 = 0.15
update = 0.01 * grad * 0.15  # Still gets 15% of update

# Over time:
# - Grad shrinks naturally (approaching local minimum)
# - Small updates still allow convergence, just slower
# - Utility decays via EMA when grad becomes very small
```

**Why more neurons activate**:

This is gradient flow from backpropagation, not gating:
```python
# As network learns task structure:
# - Error signals propagate to previously unused neurons
# - Backprop naturally spreads gradients across network
# - More neurons discover they can contribute to reducing loss

# This is a natural property of SGD + backprop:
# - Early: sparse gradient flow (few neurons active)
# - Later: distributed gradient flow (many neurons active)
```

#### The Role of Gating: Protection, Not Pattern Creation

**What gating does**:
1. **Preserves learned knowledge**: High-utility neurons (already converged) are protected from large updates
2. **Prevents catastrophic forgetting**: When task changes at step 5000, important neurons for task 1 remain stable
3. **Maintains plasticity**: Low-utility neurons can still update freely

**What gating does NOT do**:
1. Does NOT create utility redistribution (that's from gradient flow)
2. Does NOT directly affect gradient computation
3. Does NOT prevent natural convergence (just slows it for important weights)

#### Counter-Evidence: Gating Doesn't Drive Redistribution

If gating caused the utility patterns, we'd expect:
- **High-utility parameters to maintain high gradients** (because gating prevents convergence)
- **L5 to stay high** (top neurons protected at high utility)

But we observe the **opposite**:
- **L5 drops** → high utilities ARE decreasing
- **High-utility neurons ARE converging** (gradients shrinking) despite protection

This proves gating is **passive** - it responds to utility but doesn't create the redistribution pattern.

#### Thought Experiment: Gating Disabled

If we ran UPGD with `gating_factor = 1.0` (no protection):

```python
update = (grad + noise) * 1.0  # No gating
```

**Prediction**: We'd see the **same utility norm patterns**:
- L1 rises then saturates (utility tracks gradient history)
- L5 drops then saturates (redistribution from natural convergence)

**Because**: Utility is just a running average of `|grad * weight|`, computed independently of gating. The patterns emerge from how gradients naturally evolve during learning.

### Conclusion

**The utility norm patterns are caused by natural learning dynamics, not gating:**

1. **L5 drops**: Initial high-utility neurons converge → gradients shrink naturally → utility decays
2. **L1 rises**: Gradient flow spreads to more neurons → more parameters accumulate utility
3. **Both saturate**: Network reaches equilibrium on task 1

**Gating's actual role**:
- **Records importance**: Utility tracks which parameters matter (from gradient history)
- **Provides protection**: Gating uses utility to protect important (converged) parameters from forgetting
- **Enables continual learning**: When task 2 starts, high-utility neurons are shielded while low-utility neurons adapt

**Key insight**: The patterns reveal how the network learns (gradient dynamics), while gating uses those patterns to prevent forgetting (protection mechanism). Gating is a **consumer** of utility, not its **creator**.

---

## Summary of Key Findings

1. **Task Structure**: 200 permutations, each lasting 5,000 steps
2. **Utility Dynamics**: Follow learning curve (spike � decay � equilibrium)
3. **Utility Saturation**: ~49% of maximum capacity used after first task
4. **Utility Democratization**: Higher-order norms (L2, L4, L5, L10) drop while L1 rises, revealing utility redistribution from few neurons with high utility (0.8-1.0) to many neurons with moderate utility (0.4-0.7)
5. **Distribution Pattern**: Continuous spectrum of utilities [0, 1], not binary split; more neurons participating with balanced protection
6. **Gating Role**: Gating does NOT cause utility patterns; patterns emerge from natural gradient dynamics during learning. Gating uses utility to protect converged parameters from forgetting.
7. **Gating & Curvature**: Only network GateLayer affects curvature, not optimizer gating
8. **Dead Neurons**: Measured per-sample sparsity, not permanent death; ~50% ReLU sparsity contributes to ~50% average utility
9. **Plasticity**: Dead neuron ratio decreasing over training indicates healthy adaptation

---

## Code References

### Main Files Analyzed
- Task definition: `/scratch/gautschi/shin283/upgd/core/task/input_permuted_mnist.py`
- Network architecture: `/scratch/gautschi/shin283/upgd/core/network/fcn_relu.py`
- Input-aware optimizer: `/scratch/gautschi/shin283/upgd/core/optim/weight_upgd/input_aware.py`
- Training loop: `/scratch/gautschi/shin283/upgd/core/run/run_stats_with_curvature.py`
- Learner: `/scratch/gautschi/shin283/upgd/core/learner/input_aware_upgd.py`
- GateLayer: `/scratch/gautschi/shin283/upgd/core/network/gate.py`

### Key Parameters
- Learning rate: 0.01
- Sigma: 0.1
- Beta utility: 0.9999
- Weight decay: 0.01
- Seed: 2
- Network: fully_connected_relu_with_hooks



# 
now, what do I have to do? 
find the best performance hyperparameters for emnist and cifar10?
hard to find the correlation of the utility norms and curvature? Is it meaningful?

yes, I need to paraellize with reviewing the literatures and organizing. it will be my big assests.
so, set these two tasks: 1. review, and 2. think for utility and curvature and 3. experment
# UPGD Experiments - Complete Setup (Dec 16, 2025)

## Overview

This document provides complete instructions for running UPGD experiments across 4 datasets (EMNIST, Input-MNIST, CIFAR-10, Mini-ImageNet) with 8 experiment variants each, and generating publication-ready plots.

---

## 1. Experiment Scripts

### Total: 32 Slurm Scripts (8 per dataset)

All scripts are located in: `/scratch/gautschi/shin283/upgd/`

#### Script Naming Convention
- **S&P baseline**: `snp_{dataset}.sh`
- **UPGD variants**: `upgd_{dataset}_{variant}.sh`

Where `{variant}` is one of:
- `fo_global` - Full UPGD (all layers gated)
- `outputonly` - Gating only on output layer (linear_3)
- `hiddenonly` - Gating only on hidden layers (linear_1, linear_2)
- `hiddenandoutput` - Gating on all layers (same as fo_global)
- `clamped052` - Utility clamped to max 0.52
- `clamped_48_52` - Utility clamped to [0.48, 0.52]
- `clamped_44_56` - Utility clamped to [0.44, 0.56]

### Hyperparameters (From Paper's `optimizer_best_sets.csv`)

| Dataset | S&P | UPGD |
|---------|-----|------|
| **EMNIST** | lr=0.01, σ=0.01, λ=0.001 | lr=0.01, σ=0.001, β=0.9, λ=0.0 |
| **Input-MNIST** | lr=0.001, σ=0.1, λ=0.01 | lr=0.01, σ=0.1, β=0.9999, λ=0.01 |
| **CIFAR-10** | lr=0.01, σ=0.01, λ=0.001 | lr=0.01, σ=0.001, β=0.999, λ=0.0 |
| **Mini-ImageNet** | lr=0.01, σ=0.01, λ=0.001 | lr=0.01, σ=0.001, β=0.9, λ=0.0 |

**Common settings across all experiments:**
- `seed=2`
- `n_samples=1000000`
- `network=fully_connected_relu_with_hooks`
- `runner=core/run/run_stats_with_curvature.py`

---

## 2. Submitting Jobs

### Submit All EMNIST Experiments (8 jobs)
```bash
cd /scratch/gautschi/shin283/upgd

sbatch snp_emnist.sh
sbatch upgd_emnist_fo_global.sh
sbatch upgd_emnist_outputonly.sh
sbatch upgd_emnist_hiddenonly.sh
sbatch upgd_emnist_hiddenandoutput.sh
sbatch upgd_emnist_clamped052.sh
sbatch upgd_emnist_clamped_48_52.sh
sbatch upgd_emnist_clamped_44_56.sh
```

### Submit All Input-MNIST Experiments (8 jobs)
```bash
sbatch snp_imnist.sh
sbatch upgd_imnist_fo_global.sh
sbatch upgd_imnist_outputonly.sh
sbatch upgd_imnist_hiddenonly.sh
sbatch upgd_imnist_hiddenandoutput.sh
sbatch upgd_imnist_clamped052.sh
sbatch upgd_imnist_clamped_48_52.sh
sbatch upgd_imnist_clamped_44_56.sh
```

### Submit All CIFAR-10 Experiments (8 jobs)
```bash
sbatch snp_cifar10.sh
sbatch upgd_cifar10_fo_global.sh
sbatch upgd_cifar10_outputonly.sh
sbatch upgd_cifar10_hiddenonly.sh
sbatch upgd_cifar10_hiddenandoutput.sh
sbatch upgd_cifar10_clamped052.sh
sbatch upgd_cifar10_clamped_48_52.sh
sbatch upgd_cifar10_clamped_44_56.sh
```

### Submit All Mini-ImageNet Experiments (8 jobs)
```bash
sbatch snp_mini_imagenet.sh
sbatch upgd_mini_imagenet_fo_global.sh
sbatch upgd_mini_imagenet_outputonly.sh
sbatch upgd_mini_imagenet_hiddenonly.sh
sbatch upgd_mini_imagenet_hiddenandoutput.sh
sbatch upgd_mini_imagenet_clamped052.sh
sbatch upgd_mini_imagenet_clamped_48_52.sh
sbatch upgd_mini_imagenet_clamped_44_56.sh
```

### Submit All Jobs at Once (32 total)
```bash
cd /scratch/gautschi/shin283/upgd

# EMNIST
for script in snp_emnist.sh upgd_emnist_{fo_global,outputonly,hiddenonly,hiddenandoutput,clamped052,clamped_48_52,clamped_44_56}.sh; do sbatch $script; done

# Input-MNIST
for script in snp_imnist.sh upgd_imnist_{fo_global,outputonly,hiddenonly,hiddenandoutput,clamped052,clamped_48_52,clamped_44_56}.sh; do sbatch $script; done

# CIFAR-10
for script in snp_cifar10.sh upgd_cifar10_{fo_global,outputonly,hiddenonly,hiddenandoutput,clamped052,clamped_48_52,clamped_44_56}.sh; do sbatch $script; done

# Mini-ImageNet
for script in snp_mini_imagenet.sh upgd_mini_imagenet_{fo_global,outputonly,hiddenonly,hiddenandoutput,clamped052,clamped_48_52,clamped_44_56}.sh; do sbatch $script; done
```

### Monitor Jobs
```bash
squeue -u $USER
```

---

## 3. Output Data Structure

### JSON Files Location
Each experiment saves a JSON file at:
```
/scratch/gautschi/shin283/upgd/logs/{task}/{learner}/{network}/{hyperparameters}/2.json
```

### JSON File Contents

Each JSON file contains:

#### Per-Step Data (1,000,000 points)
- `losses_per_step`: Loss at each training step
- `accuracy_per_step`: Accuracy at each step
- `plasticity_per_step`: Plasticity (1 - L_after/L_before)
- `n_dead_units_per_step`: Fraction of dead neurons
- `weight_l2_per_step`: L2 norm of weights
- `weight_l1_per_step`: L1 norm of weights
- `grad_l2_per_step`: L2 norm of gradients
- `grad_l1_per_step`: L1 norm of gradients
- `grad_l0_per_step`: L0 norm of gradients

#### Utility Histogram Data (~100,000 points, logged every 10 steps)
- 9-bin scaled utility histogram (in [0, 1] after sigmoid)
- 5-bin raw utility histogram (centered at 0)
- Per-layer utility histograms for linear_1, linear_2, linear_3

---

## 4. Generating Plots

### Training Metrics Plots

```bash
cd /scratch/gautschi/shin283/upgd_plots

# Generate plots for each dataset
python scripts/plot_training_metrics.py mini_imagenet
python scripts/plot_training_metrics.py input_mnist
python scripts/plot_training_metrics.py emnist
python scripts/plot_training_metrics.py cifar10
```

#### Output: 7 plots per dataset
1. `accuracy_comparison.png`
2. `loss_comparison.png`
3. `plasticity_comparison.png`
4. `dead_units_comparison.png`
5. `weight_l2_comparison.png`
6. `weight_l1_comparison.png`
7. `grad_l2_comparison.png`

Saved to: `/scratch/gautschi/shin283/upgd_plots/figures/{dataset}/`

---

## 5. Expected JSON File Paths (by Dataset)

### EMNIST
- Task: `label_permuted_emnist_stats`
- S&P: `sgd/.../lr_0.01_sigma_0.01_beta_utility_0.9_weight_decay_0.001_n_samples_1000000/2.json`
- UPGD: `upgd_fo_global/.../lr_0.01_sigma_0.001_beta_utility_0.9_weight_decay_0.0_n_samples_1000000/2.json`

### Input-MNIST
- Task: `input_permuted_mnist_stats`
- S&P: `sgd/.../lr_0.001_sigma_0.1_beta_utility_0.9999_weight_decay_0.01_n_samples_1000000/2.json`
- UPGD: `upgd_fo_global/.../lr_0.01_sigma_0.1_beta_utility_0.9999_weight_decay_0.01_n_samples_1000000/2.json`

### CIFAR-10
- Task: `label_permuted_cifar10_stats`
- S&P: `sgd/.../lr_0.01_sigma_0.01_beta_utility_0.999_weight_decay_0.001_n_samples_1000000/2.json`
- UPGD: `upgd_fo_global/.../lr_0.01_sigma_0.001_beta_utility_0.999_weight_decay_0.0_n_samples_1000000/2.json`

### Mini-ImageNet
- Task: `label_permuted_mini_imagenet_stats`
- S&P: `sgd/.../lr_0.01_sigma_0.01_beta_utility_0.9_weight_decay_0.001_n_samples_1000000/2.json`
- UPGD: `upgd_fo_global/.../lr_0.01_sigma_0.001_beta_utility_0.9_weight_decay_0.0_n_samples_1000000/2.json`

---

## 6. Key Changes from Previous Experiments

### 1. Hyperparameters Updated to Paper Values
All S&P and UPGD hyperparameters now match `optimizer_best_sets.csv`

### 2. Script Names Updated
- Removed `test_` prefix
- Changed `sgd` to `snp` (Shrink & Perturb)
- Removed `_stats` suffix
- Format: `{method}_{dataset}_{variant}.sh`

### 3. Plotting Script Updated
`plot_training_metrics.py` now has correct paths for all 4 datasets

### 4. SGD is Actually S&P
The `sgd` learner implements **Shrink & Perturb**:
- **Shrink**: Weight decay (λ·θ)
- **Perturb**: Gaussian noise (σ·ε)
- See: `/scratch/gautschi/shin283/upgd/core/optim/sgd.py`

---

## 7. Verification & Troubleshooting

### After Jobs Complete
1. Check JSON files exist: `ls logs/{task}/{learner}/{network}/lr_*/2.json`
2. Verify file size: `du -h logs/{task}/{learner}/{network}/lr_*/2.json` (should be ~100-500 MB)
3. Check for utility histograms: `grep -o "utility_histogram_per_step" logs/{task}/{learner}/{network}/lr_*/2.json`

### If Plots Fail
1. Verify JSON paths match plotting script expectations
2. Check JSON contains `*_per_step` fields
3. Ensure seed=2 file exists (not seed=0 or seed=1)

---

## Document History

- **2025-12-16**: Complete setup for 32 experiments across 4 datasets with correct paper hyperparameters


 The utility histogram extraction script uses WandB run IDs, which will be different for the
  new experiments. After you submit the jobs, you'll need to update the run IDs in this
  script. However, the new experiments will also save utility histograms directly to JSON
  files, so you may not need WandB extraction at all.

  Let me summarize what's ready:

  Summary: Everything Updated ✅

  1. Slurm Scripts (32 total)

  All scripts have correct paper hyperparameters and use run_stats_with_curvature.py which
  saves:
  - Per-step metrics to JSON (1M points)
  - Utility histograms to JSON (9 bins, ~100k points)
  - Per-layer utility histograms

  2. Plotting Scripts Updated

  plot_training_metrics.py - Updated with correct paths for all 4 datasets:
  - Mini-ImageNet ✅
  - Input-MNIST ✅
  - EMNIST ✅
  - CIFAR-10 ✅

  3. After Experiments Complete

  Once experiments finish, you can generate plots:
  cd /scratch/gautschi/shin283/upgd_plots

  # Training metrics plots
  python scripts/plot_training_metrics.py mini_imagenet
  python scripts/plot_training_metrics.py input_mnist
  python scripts/plot_training_metrics.py emnist
  python scripts/plot_training_metrics.py cifar10

  4. Utility Histogram Plots

  For utility histograms, you have two options:
  1. Use JSON data (new experiments save it automatically) - update plot_utility_histograms.py
   to read from JSON files instead of WandB
  2. Use WandB - update run IDs in extract_utility_histograms_local.py after experiments
  complete

  Ready to Submit!

  cd /scratch/gautschi/shin283/upgd

  # Mini-ImageNet (8 jobs)
  sbatch snp_mini_imagenet.sh
  sbatch upgd_mini_imagenet_fo_global.sh
  sbatch upgd_mini_imagenet_outputonly.sh
  sbatch upgd_mini_imagenet_hiddenonly.sh
  sbatch upgd_mini_imagenet_hiddenandoutput.sh
  sbatch upgd_mini_imagenet_clamped052.sh
  sbatch upgd_mini_imagenet_clamped_48_52.sh
  sbatch upgd_mini_imagenet_clamped_44_56.sh

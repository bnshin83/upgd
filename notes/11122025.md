# Analysis of Scaled Utility Distribution - November 12, 2025

## Observation
Histograms show scaled utilities cluster in the middle range [0.4, 0.6] rather than spanning the full [0, 1] range.

## Mathematical Analysis

### Utility Calculation
The utility metric is computed as:
$$\bar{U}_t = \beta \cdot \bar{U}_{t-1} + (1-\beta) \cdot u_t$$

where the instantaneous utility is:
$$u_t = -\nabla \mathcal{L} \cdot w$$

### Scaled Utility Computation
$$\text{scaled\_utility} = \sigma\left(\frac{\bar{U}/\text{bias\_correction}}{\text{global\_max\_util}}\right)$$

where $\sigma(x) = \frac{1}{1+e^{-x}}$ is the sigmoid function.

### Empirical Values from Experiments

**Network parameters:**
- Total parameters: 280,000
- Weight L1 norm: $\|w\|_1 \approx 10{,}000$
- Gradient L1 norm: $\|\nabla\mathcal{L}\|_1 \approx 8{,}000$
- Global max utility: $0.001$ to $0.005$

**Average magnitudes:**
$$\text{avg}|w| \approx \frac{10{,}000}{280{,}000} \approx 0.036$$
$$\text{avg}|\nabla\mathcal{L}| \approx \frac{8{,}000}{280{,}000} \approx 0.029$$

### Expected Utility Magnitude

Element-wise product:
$$-\nabla\mathcal{L} \cdot w \approx \pm(0.036 \times 0.029) \approx \pm 0.001$$

With mixed signs and EMA smoothing, individual utilities range:
$$\bar{U} \in [-0.002, 0.003]$$

### Why Utilities Cluster at [0.4, 0.6]

With $\text{global\_max\_util} \approx 0.003$, the normalized values are:

| Parameter | $\bar{U}$ | Normalized | Sigmoid | Interpretation |
|-----------|-----------|------------|---------|----------------|
| Low utility | $-0.002$ | $-0.67$ | $\sigma(-0.67) \approx 0.34$ | Less protected |
| Zero utility | $0.0$ | $0$ | $\sigma(0) = 0.50$ | Neutral |
| Med utility | $0.001$ | $0.33$ | $\sigma(0.33) \approx 0.58$ | Slightly protected |
| Max utility | $0.003$ | $1.0$ | $\sigma(1.0) \approx 0.73$ | Most protected |

**Conclusion:** Most parameters have utilities near zero, which when normalized by the small $\text{global\_max\_util}$ yield inputs near 0 to the sigmoid. Since $\sigma(0) = 0.5$, the distribution naturally clusters around [0.4, 0.6].

## Theoretical Range of Scaled Utility

Since utilities are normalized by $\text{global\_max\_util}$:
- **Maximum input to sigmoid:** $\approx 1$ (for highest utility parameter)
- **Minimum input to sigmoid:** $-\infty$ (for very negative utilities)

Therefore:
$$\text{scaled\_utility} \in (0, \sigma(1)] \approx (0, 0.73]$$

The theoretical upper bound is $\sigma(1) \approx 0.73$, **not 1.0**.

## Implications

1. **Not a bug:** The clustering at [0.4, 0.6] is mathematically expected given the small scale of utilities
2. **Relative differences matter:** Even though absolute values are small, parameters with utility $0.003$ (scaled $\approx 0.73$) are still differentiated from those with utility $-0.002$ (scaled $\approx 0.34$)
3. **Gating still functional:** The gating term $(1 - \text{scaled\_utility})$ ranges from $\approx 0.27$ to $\approx 0.66$, providing meaningful protection variation

## Alternative Scaling Methods

The current sigmoid-based scaling limits the output range to $(0, 0.73]$ instead of the full $[0, 1]$. Here are alternative methods to achieve full-range scaling:

### Option 1: Min-Max Normalization
$$\text{scaled\_utility} = \frac{\bar{U}/\text{bias\_correction} - \text{global\_min\_util}}{\text{global\_max\_util} - \text{global\_min\_util}}$$

**Pros:**
- Output range: exactly $[0, 1]$
- Linear mapping preserves relative distances
- Intuitive interpretation

**Cons:**
- Requires tracking both min and max utilities
- Sensitive to outliers
- No smoothing/squashing of extreme values

**Implementation:**
```python
# First pass: track both min and max
global_min_util = torch.tensor(torch.inf)
global_max_util = torch.tensor(-torch.inf)
for ...:
    current_util = state["avg_utility"] / bias_correction
    global_min_util = torch.min(global_min_util, current_util.min())
    global_max_util = torch.max(global_max_util, current_util.max())

# Second pass: normalize
scaled_utility = (state["avg_utility"] / bias_correction - global_min_util) / (global_max_util - global_min_util + 1e-8)
```

### Option 2: Clipped Linear Normalization
$$\text{scaled\_utility} = \text{clip}\left(\frac{\bar{U}/\text{bias\_correction}}{\text{global\_max\_util}}, 0, 1\right)$$

**Pros:**
- Output range: exactly $[0, 1]$
- Simple implementation
- Only requires tracking max (assumes utilities ≥ 0)

**Cons:**
- Hard clipping at 0 and 1 (discontinuous derivative)
- Assumes non-negative utilities
- All negative utilities mapped to 0

**Implementation:**
```python
scaled_utility = torch.clamp((state["avg_utility"] / bias_correction) / global_max_util, 0, 1)
```

### Option 3: Softmax Normalization
$$\text{scaled\_utility}_i = \frac{\exp(\bar{U}_i / (τ \cdot \text{bias\_correction}))}{\sum_j \exp(\bar{U}_j / (τ \cdot \text{bias\_correction}))}$$

**Pros:**
- Output range: $(0, 1)$, with $\sum_i \text{scaled\_utility}_i = 1$
- Creates competition between parameters
- Temperature $τ$ controls sharpness

**Cons:**
- Changes interpretation (now relative importance, not absolute)
- Computationally expensive (requires full normalization)
- Scale depends on number of parameters

**Implementation:**
```python
all_utilities = torch.cat([state["avg_utility"] / bias_correction for ... ])
scaled_utilities = torch.softmax(all_utilities / temperature, dim=0)
```

### Option 4: Tanh Rescaled
$$\text{scaled\_utility} = \frac{\tanh\left(\frac{\bar{U}/\text{bias\_correction}}{\text{global\_max\_util}}\right) + 1}{2}$$

**Pros:**
- Output range: $(0, 1)$
- Smooth, differentiable everywhere
- $\tanh(1) \approx 0.76$ rescales to $0.88$ (better than sigmoid's $0.73$)
- Symmetric around 0

**Cons:**
- Still doesn't use full $[0, 1]$ range
- More complex than clipping
- Max value $\approx 0.88$, not 1.0

**Implementation:**
```python
scaled_utility = (torch.tanh((state["avg_utility"] / bias_correction) / global_max_util) + 1) / 2
```

### Comparison Table

| Method | Output Range | Max Value | Smoothness | Complexity | Full Range? |
|--------|--------------|-----------|------------|------------|-------------|
| **Current (Sigmoid)** | $(0, 0.73]$ | $0.73$ | Smooth | Low | ❌ |
| **Min-Max** | $[0, 1]$ | $1.0$ | Linear | Medium | ✅ |
| **Clipped** | $[0, 1]$ | $1.0$ | Discontinuous | Low | ✅ |
| **Softmax** | $(0, 1)$ | Varies | Smooth | High | ✅ |
| **Tanh** | $(0, 1)$ | $0.88$ | Smooth | Low | ❌ |

### Recommendation

**Best options for full [0, 1] range:**
1. **Min-Max** (Option 1): Best for preserving relative utility differences
2. **Clipped** (Option 2): Simplest, good if utilities are naturally non-negative

**For experimental comparison:**
Test both Min-Max and Clipped to see if using the full [0, 1] range improves performance.

## Update Equations Comparison

### Shrink and Perturb
$$w \leftarrow w \cdot (1 - \alpha \beta) - \alpha \cdot (\nabla\mathcal{L} + \xi)$$

### SGD (with weight decay)
$$w \leftarrow w - \alpha \cdot (\nabla\mathcal{L} + \xi + \beta \cdot w)$$

**Mathematical equivalence:** Both are identical when expanded.

### UPGD (First Order Global)
$$w \leftarrow w \cdot (1 - \alpha \beta) - \alpha \cdot (\nabla\mathcal{L} + \xi) \cdot (1 - \bar{U})$$

**Key difference:** UPGD adds utility-based gating $(1 - \bar{U})$ to selectively protect high-utility parameters.

# UPGD Implementation Discussion Summary - November 6, 2025

## Overview
Analysis of UPGD (Utility-based Perturbed Gradient Descent) optimizer implementation, focusing on understanding the utility mechanism, parameter dimensions, and relationship between input curvature and utility for continual learning.

---

## 1. Optimizer State Management

### Parameter Groups (`self.param_groups`)
Inherited from `torch.optim.Optimizer`, contains:
- **`params`**: List of parameter tensors
- **`names`**: List of parameter names
- **Scalar hyperparameters**:
  - `lr`: Learning rate
  - `weight_decay`: L2 regularization
  - `beta_utility`: EMA momentum for utility (e.g., 0.9)
  - `sigma`: Noise standard deviation

### Per-Parameter State (`self.state[p]`)
For standard UPGD (first_order.py):
- `step`: Number of optimization steps
- `avg_utility`: Exponential moving average of utility

For input-aware UPGD (input_aware.py):
- `step`: Number of optimization steps
- `avg_utility`: Exponential moving average of utility
- `initial_params`: Initial parameter values (for EWC-like regularization)

---

## 2. Network Architecture: FullyConnectedReLUWithHooks

### Architecture (n_hidden_units=300)
```
Input (784) → Linear_1 (300) → ReLU → Linear_2 (150) → ReLU → Linear_3 (47) → Output
```

### Parameter Dimensions for EMNIST
- **Input**: n_obs = 784 (28×28 flattened images)
- **Output**: n_outputs = 47 (EMNIST classes)
- **Hidden**: n_hidden_units = 300

**Parameters:**
1. `linear_1.weight`: [300, 784] = 235,200 params
2. `linear_1.bias`: [300] = 300 params
3. `linear_2.weight`: [150, 300] = 45,000 params
4. `linear_2.bias`: [150] = 150 params
5. `linear_3.weight`: [47, 150] = 7,050 params
6. `linear_3.bias`: [47] = 47 params

**Total: 287,747 parameters**

### Weight Initialization
Uses PyTorch's `reset_parameters()`:
- **Weights**: Kaiming uniform initialization (He initialization)
  - Uniform distribution: U(-√k, √k) where k = 1/fan_in
- **Biases**: Uniform in [-1/√fan_in, 1/√fan_in]

**NOT zero-initialized** - random initialization is critical for non-zero initial utility.

---

## 3. Utility Mechanism

### Utility Definition
```
u_t = -g_t ⊙ θ_t
```
Where:
- **g_t**: Gradient from current sample (current information)
- **θ_t**: Current parameter value (historical information)
- **⊙**: Element-wise multiplication

### Interpretation
**High utility** = Large weight AND large gradient → Important for both:
- Past learning (large θ from previous updates)
- Current sample (large |g| from current gradient)

### Exponential Moving Average Update
```python
avg_utility.mul_(beta_utility).add_(-p.grad.data * p.data, alpha=1 - beta_utility)
```

Equivalent to:
```
U_t = β·U_{t-1} + (1-β)·(-g_t ⊙ θ_t)
```

**First sample (t=0):**
- U_0 = 0·β + (1-β)·(-g_0 ⊙ θ_0) = (1-β)·(-g_0 ⊙ θ_0)
- Uses initialized weights and first gradient
- **Utility exists from the first sample!**

### Bias Correction
```
bias_correction = 1 - β^t
corrected_avg = U_t / bias_correction
```

Compensates for zero initialization bias (similar to Adam optimizer).

---

## 4. Per-Weight Granularity

### Key Insight
`scaled_utility` has the **same dimensions as each parameter tensor**.

**Example for linear_1.weight [300, 784]:**
- 235,200 individual utility values
- Each weight element has its own importance score
- Element-wise gating provides fine-grained control

### Update Loop
```python
for name, p in zip(group["names"], group["params"]):
    # p changes shape each iteration
    # Iteration 1: p = linear_1.weight [300, 784]
    # Iteration 2: p = linear_1.bias [300]
    # ... etc for all 6 parameter tensors
```

**Per sample:** All 287,747 parameters updated once.

---

## 5. Gating Mechanism

### Standard UPGD (first_order.py)
```python
gating_factor = 1 - scaled_utility
```
- High utility → Low gating → **Small updates** (protection)
- Low utility → High gating → **Large updates** (plasticity)

### Input-Aware UPGD (input_aware.py)

**Option A: Linear clamp**
```python
gating_factor = clamp(1 - scaled_utility * lambda_reg, min=0.0)
```

**Option C: Rational function**
```python
gating_factor = 1 / (1 + a*lambda_reg + b*scaled_utility)
gating_factor = clamp(gating_factor, min=min_g, max=1.0)
```

Where:
- **λ (lambda_reg)**: Dynamic protection based on input curvature κ
- **scaled_utility**: Parameter importance ∈ [0,1]

---

## 6. Input Curvature and Lambda Mapping

### Curvature Computation
Uses finite differences with Rademacher vectors (from `compute_input_curvature_finite_diff`):
- **Per-sample curvature**: κ_t (scalar value per sample)
- Represents sample difficulty/distinctiveness

### Lambda Mapping Functions
Maps curvature κ to regularization strength λ:

**1. Sigmoid (default):**
```
λ = λ_max · σ((κ - τ) / s)
```

**2. Centered Linear:**
```
λ = clamp(1 + (κ - τ)/s, 0, λ_max)
```

**3. Tau Ratio (parameter-free):**
```
λ = clamp(κ / τ, 0, λ_max)
```

**4. Sigmoid Tau Ratio (parameter-free):**
```
λ = λ_max · σ((κ - τ) / τ)
```

Where:
- **τ (curvature_threshold)**: Baseline curvature (e.g., 0.01)
- **s (lambda_scale)**: Scaling factor (e.g., 0.1)
- **λ_max**: Maximum protection strength (e.g., 2.0)

---

## 7. Research Hypothesis: Curvature-Utility Correlation

### Hypothesis
**High curvature samples activate more/higher utilities**

**Rationale:**
- **Low curvature (easy) samples**: Few important weights needed
- **High curvature (hard) samples**: Many important weights coordinated → higher utility activation

### Per-Sample Dimensions
- **Curvature**: 1 scalar value (κ)
- **Utility**: 287,747 values (one per weight)

### Proposed Aggregations for Correlation Analysis

**1. Central tendency:**
- Mean utility: `mean(U)`
- Median utility: `p50(U)`

**2. Tail behavior (robust to skewness):**
- Percentiles: p75, p90, p95, p99, p99.9
- Max utility: `max(U)`

**3. Distribution shape:**
- Skewness: asymmetry measure
- Kurtosis: tail heaviness
- Fraction in top 1%/5%/10%
- Gini coefficient: inequality measure

**4. Spread measures:**
- IQR (p75 - p25): robust spread
- MAD (median absolute deviation)
- Standard deviation

**5. Total activation:**
- **Sum of utilities**: Σ(U) - total importance activated
- **Sum of gating**: Σ(g) - total gate openness
  - 0 = all gates closed (full protection)
  - 287,747 = all gates open (full plasticity)

### Expected Correlation
If hypothesis holds:
- **Positive correlation**: κ vs mean(U), κ vs Σ(U)
- **Negative correlation**: κ vs mean(gating), κ vs Σ(gating)

High-κ samples should show:
- Higher utility activation → More protection needed
- Lower gating values → Less plasticity (stability)

---

## 8. Gating Factor Analysis

### Metrics to Track Per-Sample

**1. Gating statistics (287,747 values per sample):**
- Mean gating: average openness
- Percentiles (p50, p90, p99): distribution
- Fraction fully open (g=1): % weights with no protection
- Fraction fully closed (g≈0): % weights fully protected

**2. Effective update magnitude:**
```
||Δθ||² = ||lr * gating * (grad + noise)||²
```

**3. Correlations:**
- κ vs mean(gating): High curvature → lower gating?
- κ vs fraction(g<0.1): More weights protected for high-κ?
- utility vs gating: Direct relationship validation

### Interpretation
Reveals **plasticity vs stability tradeoff** per sample:
- High Σ(gating) → More plasticity
- Low Σ(gating) → More stability/protection

---

## 9. Statistical Considerations

### Distribution Challenges
With 287,747 weights, utility distribution likely:
- **Highly skewed**: Few high-utility weights, many low-utility
- **Long-tailed**: Extreme values possible
- **Not normal**: Standard statistics may be misleading

### Robust Analysis Approach

**1. Visualizations:**
- Histogram (linear and log-scale)
- CDF plot (shows full distribution)
- Q-Q plot (test normality assumption)
- Scatter: κ vs percentiles

**2. Robust correlations:**
- **Spearman correlation**: Rank-based, robust to outliers
- Correlation with multiple percentiles
- Tail ratio analysis: p99/p50 ratio vs κ

**3. Stratified analysis:**
- Bin samples by curvature (low/medium/high κ)
- Compare utility distributions across bins
- Statistical tests (Mann-Whitney U, Kruskal-Wallis)

---

## 10. Implementation Summary

### Streaming Learning Setup
For each training sample:
1. **Forward pass**: Compute predictions with current weights
2. **Compute loss**: Compare to target
3. **Backward pass**: Compute gradients for all parameters
4. **optimizer.step()**:
   - Loop through all 287,747 parameters
   - Update utility EMA for each
   - Compute per-weight gating
   - Apply gated update
5. **Next sample**: Uses updated weights

### Skip Logic
```python
if p.grad is None or 'gate' in name:
    continue
```
Skips parameters with:
- No gradient (not used in backward pass)
- 'gate' in name (gating mechanism parameters)

---

## 11. Next Steps for Analysis

### Code Modifications Needed
1. Add per-sample utility statistics logging:
   - Percentiles, mean, std, skewness, kurtosis
   - Sum of utilities, sum of gating factors

2. Track gating distribution per sample:
   - Mean, percentiles, fraction protected/open

3. Compute correlations:
   - κ vs utility aggregates
   - κ vs gating aggregates
   - Use Spearman correlation for robustness

### Expected Files to Modify
- `core/run/run_stats_with_curvature.py`: Add logging
- `core/optim/weight_upgd/input_aware.py`: Return utility/gating stats
- Analysis scripts: Plot correlations and distributions

### Validation Questions
1. Does high κ correlate with high utility activation?
2. Is utility distribution long-tailed/skewed?
3. Do high-κ samples have more protected weights (lower gating)?
4. Does the λ(κ) mapping create appropriate protection levels?

---

## Key Takeaways

1. **Per-weight granularity**: 287,747 individual utility values per sample
2. **Utility combines**: Current gradient (current info) × Weight value (historical info)
3. **Gating inverts utility**: High utility → Low gating → Protection
4. **Input curvature modulates**: λ(κ) scales protection based on sample difficulty
5. **Hypothesis**: High-κ samples activate more utility → Need more protection
6. **Statistical care needed**: Long-tailed distributions require robust statistics
7. **Sum of gating**: Direct measure of total plasticity per sample

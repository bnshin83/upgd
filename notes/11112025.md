# November 11, 2025 - Research Notes

## Utility Histogram Analysis Across Optimizers

### Experimental Setup
Running 4 jobs in parallel:
1. SGD (no gating)
2. UPGD first-order (gating on gradient + noise)
3. UPGD nonprotecting (gating only on noise)
4. Input-aware UPGD (dynamic gating based on input curvature)

All experiments use:
- Same model architecture
- `beta_utility = 0.9999` (very slow EMA)
- Input Permuted MNIST task
- 1,000,000 samples total
- Task changes every 5,000 samples

### Key Observation: Identical Utility Histograms

**Initial puzzle**: All 4 optimizers show identical utility histograms in WandB.

**Resolution**: This makes perfect sense because:
- All optimizers compute utility identically: `utility = -grad * param`
- Same model � same gradients and parameters � same utility distribution
- Utility histogram measures **parameter importance**, not optimizer behavior
- The difference lies in **how utilities are used** (gating), not in the utilities themselves

### Critical Discovery: The Balancing Phase

**Utility histogram evolution**:
- **First 300 samples** (6% of first task): Utilities rapidly balance/converge to middle range (40-60% bin)
- **After 300 samples**: Utilities remain relatively stable around 40-60%

**Key insight**: This early balancing phase appears to be **critically important** for continual learning performance.

### Performance Difference Hypothesis

Even though utility distributions are identical, performance differs dramatically:

#### 1. UPGD (with gating): Best Performance
- Update rule: `param -= lr * (grad + noise) * (1 - utility)`
- **During balancing phase (0-300 samples)**:
  - Gating factor `(1 - utility)` protects parameters as they emerge as important
  - High utility params (becoming important) get small updates � preserved for future tasks
  - Low utility params get large updates � still plastic and adaptable
- **After balancing**:
  - Utilities stabilize at ~0.5 � gating factor ~0.5 for most params
  - Protection mechanisms remain active

#### 2. SGD (no gating): Worst Performance
- Update rule: `param -= lr * grad`
- **During balancing phase**:
  - Updates all parameters equally regardless of importance
  - **Misses the critical protection** of emerging important parameters
  - Potentially damages parameters that should be preserved
- **After balancing**:
  - Continues updating without regard to parameter importance
  - Catastrophic forgetting more likely

#### 3. UPGD Nonprotecting: Intermediate
- Update rule: `param -= lr * (grad + noise * (1 - utility))`
- Gating applied only to noise, not gradient
- Shows that even partial protection helps

#### 4. Input-aware UPGD: Adaptive
- Update rule: `param -= lr * (grad + regularization + noise) * clamp(1 - utility * lambda)`
- Dynamic protection based on input curvature
- Additional regularization term pulls important params toward initialization

### The Central Hypothesis

**"Something very important happens during the first 300 samples (the balancing phase)."**

If an optimizer fails to leverage parameter importance during this critical transient period (like SGD), it suffers in performance for the entire task and subsequent tasks. The early phase is where the optimizer must:

1. Identify which parameters are becoming important for the current task
2. Protect those parameters from excessive updates
3. Allow less important parameters to remain plastic

**UPGD's advantage**: The gating mechanism `(1 - utility)` is most effective during this balancing phase when utility gradients are steepest and parameter roles are being established.

**SGD's disadvantage**: By treating all parameters equally, it "misses" the opportunity to establish proper protection during this critical window.

### Implications for Understanding UPGD

1. **Utility histograms alone don't explain performance** - must look at gating factors and effective learning rates

2. **Early phase is critical** - the first 6% of a task may determine protection patterns for the remaining 94%

3. **Dynamic vs static approaches**:
   - UPGD uses same gating throughout but benefits from early establishment
   - Input-aware UPGD adapts protection dynamically based on task difficulty

4. **Why beta_utility = 0.9999 works well**:
   - Very slow EMA preserves long-term parameter importance
   - Allows utilities to stabilize and maintain protection across tasks

### Next Steps for Investigation

1. **Verify the timing hypothesis**:
   - Does UPGD's accuracy gain over SGD happen primarily in first 300 samples?
   - Does the performance gap remain constant after balancing?

2. **Analyze gating factor histograms**:
   - Track `(1 - utility)` distribution over time
   - Compare effective learning rates across optimizers

3. **Study parameter-level dynamics**:
   - Which specific parameters are protected during balancing?
   - How do protection patterns evolve across task boundaries?

4. **Test intervention experiments**:
   - What if SGD is replaced with UPGD only during first 300 samples?
   - What if UPGD gating is disabled after balancing phase?

---

## Experimental Results: SGD Half-LR Outperformed UPGD

### Surprising Finding
**SGD with lr=0.005 (half learning rate) outperformed UPGD with lr=0.01 + utility gating**

This suggests that UPGD's utility-based gating `(1 - utility)` may be:
- Too aggressive (after utilities stabilize at ~0.5, effective lr becomes ~0.005)
- Not adaptive enough to task difficulty
- Missing important information about input space geometry

### Hypothesis: Input Curvature is More Informative
If utility-based protection is too rigid, perhaps **input curvature** provides better signal for when to protect vs update:
- High input curvature = difficult/sharp loss landscape = need protection
- Low input curvature = smooth loss landscape = safe to update aggressively

---

## New Experiments: Testing Curvature-Based Gating

### Current Test Suite (Input Permuted MNIST)

All experiments use:
- Network: `fully_connected_relu_with_hooks`
- Task: `input_permuted_mnist_stats`
- Seed: 2
- Samples: 1,000,000
- Weight decay: 0.01
- Beta utility: 0.9999 (for monitoring)

#### Experiment 1: Vanilla SGD (Baseline)
**Script:** `test_sgd_input_mnist_stats.sh` (existing, seed 0)
```
lr = 0.01
sigma = 0.0 (no noise)
weight_decay = 0.0001
```
**Purpose:** Original baseline without matching UPGD's hyperparameters

#### Experiment 2: SGD with Half Learning Rate
**Script:** `test_sgd_half_lr_input_mnist_stats.sh`
```
lr = 0.005 (half of UPGD)
sigma = 0.1 (matching UPGD noise)
weight_decay = 0.01 (matching UPGD)
Update: param -= lr * (grad + noise + weight_decay * param)
```
**Purpose:** Test if UPGD's advantage is just effective LR reduction
**Result:** **Outperformed UPGD** - suggesting utility-gating is too aggressive

#### Experiment 3: SGD with Curvature-Based Gating (NEW)
**Script:** `test_sgd_curvature_gating_input_mnist_stats.sh`
```
lr = 0.01
sigma = 0.1 (noise)
weight_decay = 0.01
curvature_threshold (τ) = 0.01
curvature_scale (s) = 0.01
beta_curvature = 0.9

Update: param -= lr * (grad + noise + weight_decay * param) * g
where: g = sigmoid(-(κ - τ) / s)
       κ = input curvature (computed via finite differences)
```

**Gating equation (Option D - Inverse Sigmoid):**
```
g = sigmoid(-(κ - τ) / s)
```

**Behavior:**
- When κ = 0.0 (very low curvature): g ≈ 0.73 → mostly full update
- When κ = 0.01 (at threshold): g = 0.5 → half update
- When κ = 0.02 (high curvature): g ≈ 0.27 → strong protection
- When κ = 0.05 (very high): g ≈ 0.02 → very strong protection

**Purpose:** Test if curvature-based gating is more adaptive than utility-based gating

**Advantages over utility-based gating:**
1. **Input-aware**: Responds to task difficulty in real-time
2. **Adaptive transition**: Sharp scale (0.01) means quick response to curvature changes
3. **Global signal**: Single gating factor for all parameters (simpler than per-parameter utility)
4. **Task-aligned**: Protects when loss landscape is sharp, updates when smooth

#### Experiment 4: UPGD First-Order (Comparison)
**Script:** `test_upgd_fo_global_input_mnist_stats.sh`
```
lr = 0.01
sigma = 0.1
weight_decay = 0.01
beta_utility = 0.9999

Update: param -= lr * (grad + noise) * (1 - utility)
        utility = -grad * param (tracked with EMA)
```
**Purpose:** Original UPGD for comparison

### Key Comparisons

| Optimizer | Gating Mechanism | Gating Adaptivity | Effective LR |
|-----------|------------------|-------------------|--------------|
| SGD (vanilla) | None | N/A | 0.01 (constant) |
| SGD (half-lr) | None | N/A | 0.005 (constant) |
| UPGD | `(1 - utility)` | Slow (β=0.9999) | ~0.005 (after stabilization) |
| SGD Curvature Gating | `sigmoid(-(κ-τ)/s)` | Fast (real-time κ) | Dynamic [0.01, ~0] |

### Predictions

**If curvature-gating outperforms SGD half-lr:**
- Confirms that **adaptive protection based on task difficulty** is key
- Suggests utility alone is insufficient (needs input-space information)
- Opens path for hybrid approaches (utility + curvature)

**If curvature-gating ≈ SGD half-lr:**
- Protection mechanism matters less than we thought
- Simple LR reduction might be sufficient for continual learning
- Questions the value of complex gating strategies

**If curvature-gating < SGD half-lr:**
- Input curvature might be noisy or not well-aligned with forgetting
- Scale parameters (τ=0.01, s=0.01) might need tuning
- Consider other gating strategies (Option A/B/C)
